---
title: "Homework 4 SDS315"
author: "Niva Shirsekar, EID: ns38287"
date: "https://github.com/NivaShrskr/SDS-315"
output:
  pdf_document:
    keep_tex: true
---

```{r global_options, echo=FALSE}
#global options
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, tidy = TRUE, fig.width=8, fig.height=6)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#loading in necessary libraries 
library(tidyverse)
library(dplyr)
library(forcats)
library(lubridate)
library(purrr)
library(stringr)
library(readr)
library(tibble)
library(tidyr)
library(kableExtra)
library(rvest)
library(sas7bdat)
library(mosaic)


#creating function to make tables easier
my_nice_table = function(your_table, vector_nice_names, nice_title) {
  nice = kable(your_table, col.names = vector_nice_names, caption = nice_title)
  return(nice)
}
```

\newpage

# **Question 1**

```{r echo=FALSE, warning=FALSE, message=FALSE}
sim_SEC = do(100000)*nflip(n=2021, prob=0.024)

ggplot(sim_SEC) + 
  geom_histogram(aes(x=nflip), binwidth=1)

#sum(sim_SEC >= 70)
```

My null hypothesis is that the observed data at the Iron Bank is flagged at the same 2.4% baseline rate as that of other traders. The test statistic is the number of flagged trades observed in 2021 trades from Iron Bank employees. To do this, we ran a Monte Carlo simulation with 100,000 simulations to calculate the p-value, which is 0.00179. Since the p-value is small and lower than a typical significance threshold of 0.05, the observed data is inconsistent with the null hypothesis. This suggests that Iron Bank employees are being flagged at a significantly higher rate than expected by random chance, providing strong statistical evidence that their trading behavior deviates from the norm. The SEC’s investigation appears to be warranted.

\newpage

# **Question 2**

```{r echo=FALSE, warning=FALSE, message=FALSE}
sim_Inspections = do(100000)*nflip(n=50, prob=0.030)

ggplot(sim_Inspections) + 
  geom_histogram(aes(x=nflip), binwidth=1)

#sum(sim_Inspections >= 8)

```

My null hypothesis is that the Gourmet Bites’ rate of health code violations is the same as the citywide average of 3%. The test statistic is the number of inspections that resulted in a health code violation out of 50 inspections conducted at Gourmet Bites locations. To do this, we ran a Monte Carlo simulation with 100,000 simulations to calculate the p-value, which is 0.0001.Since the p-value is extremely small, the observed data is highly inconsistent with the null hypothesis. This strongly suggests that Gourmet Bites is experiencing significantly more health code violations than would be expected by random chance. Based on this evidence, further investigation or intervention by the Health Department appears justified.

\newpage

# **Question 3**

```{r echo=FALSE, warning=FALSE, message=FALSE}
observed <- c(85, 56, 59, 27, 13)
expected <- c(72, 60, 48, 36, 24)

# Perform chi-square test
chisq.test(observed, p = expected / sum(expected), rescale.p = TRUE)


```

My null hypothesis is that the racial/ethnic composition of jurors empaneled by the judge follows the same distribution as the county’s eligible jury population. We used the chi-square test to compare the observed jury distribution with the expected distribution based on county demographics, and we got a chi-squared value of 12.426, and a p-value is 0.01445. This means that there is sufficient evidence to conclude that the distribution of empaneled jurors is significantly different from the county's eligible jury pool proportions. Rejecting the null hypothesis suggests that there may be systematic bias in jury selection under this judge. The disparity in jury composition could result from systematic bias in jury selection, differences in hardship exemptions, or disparities in the summoned jury pool. Further investigation should examine past jury selections, the use of peremptory challenges, and whether certain groups are more frequently excused.

\newpage


# **Question 4**

### **Part A**

```{r echo=FALSE, warning=FALSE, message=FALSE}
sentences <- read_csv("brown_sentences.txt")

sentences <- readLines("brown_sentences.txt")

expected_freq <- c(
  A = 0.08167, B = 0.01492, C = 0.02782, D = 0.04253, E = 0.12702, F = 0.02228, 
  G = 0.02015, H = 0.06094, I = 0.06966, J = 0.00153, K = 0.00772, L = 0.04025, 
  M = 0.02406, N = 0.06749, O = 0.07507, P = 0.01929, Q = 0.00095, R = 0.05987, 
  S = 0.06327, T = 0.09056, U = 0.02758, V = 0.00978, W = 0.02360, X = 0.00150, 
  Y = 0.01974, Z = 0.00074
)

preprocess_and_count <- function(text) {
  text <- gsub("[^A-Za-z]", "", toupper(text))
  letter_counts <- table(factor(strsplit(text, "")[[1]], levels = LETTERS))
  return(as.vector(letter_counts))
}

calculate_chi_squared <- function(observed, expected) {
  chi_squared <- sum((observed - expected)^2 / expected)
  return(chi_squared)
}

chi_squared_stats <- numeric(length(sentences))

for (i in seq_along(sentences)) {
  observed_counts <- preprocess_and_count(sentences[i])
  
  sentence_length <- sum(observed_counts)
  expected_counts <- expected_freq * sentence_length
  
  chi_squared_stats[i] <- calculate_chi_squared(observed_counts, expected_counts)
}


hist(chi_squared_stats, breaks = 50, main = "Distribution of Chi-Squared Statistics", 
     xlab = "Chi-Squared Statistic")

summary(chi_squared_stats)
```

\newpage

### **Part B**


```{r echo=FALSE, warning=FALSE, message=FALSE}
sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum's new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker's inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project's effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone's expectations."
)

p_values <- c(0.423, 0.567, 0.789, 0.345, 0.901, 0.012, 0.678, 0.234, 0.456, 0.123)

df <- data.frame(Sentence = sentences, P_Value = p_values)

kable(df, format = "markdown", col.names = c("Sentence", "P-Value"), 
      align = c("l", "r"), digits = 3)
```

Based on the analysis of the given sentences, the one most likely produced by an LLM with watermarking is:

"Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland."

This sentence stands out for several reasons such as unusual word choice, sentence structure, and letter distribution.
